{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "def update_working_directory():\n",
    "    from pathlib import Path\n",
    "\n",
    "    p = Path(os.getcwd()).parents[0]\n",
    "    os.chdir(p)\n",
    "    print(p)\n",
    "\n",
    "\n",
    "update_working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path=\"data/raw/german_english.csv\", list_columns=None):\n",
    "    vocab = pd.read_csv(vocab_path)\n",
    "\n",
    "    if list_columns is None:\n",
    "        list_columns = [\"id_vocab\", \"german\", \"english\"]\n",
    "    vocab = vocab[list_columns]\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(\"data/raw/german_english__feature.csv\")\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_vocab_features import create_vocab_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_test = create_vocab_features(vocab)\n",
    "vocab_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Levenshtein distance between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_article(\n",
    "    vocab,\n",
    "    list_german_article = ['der','die','das'],\n",
    "    list_english_article = ['the','to']\n",
    "):\n",
    "\n",
    "    vocab['german'] = vocab['german'].map(\n",
    "        lambda x: ' '.join(word for word in x.split(' ') if word not in list_german_article)\n",
    "    )\n",
    "    vocab['english'] = vocab['english'].map(\n",
    "        lambda x: ' '.join(word for word in x.split(' ') if word not in list_english_article)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_levenshtein_distance(vocab):\n",
    "    \n",
    "    from Levenshtein import distance\n",
    "    \n",
    "    vocab = vocab.copy()\n",
    "    \n",
    "    # Lowercase\n",
    "    vocab['german'] = vocab['german'].str.lower()\n",
    "    vocab['english'] = vocab['english'].str.lower()\n",
    "    \n",
    "    # Remove article\n",
    "    remove_article(vocab)\n",
    "    \n",
    "    # Calculate Levenshtein distance\n",
    "    levenshtein_dist = vocab.apply(lambda x: distance(x['german'], x['english']), axis=1)\n",
    "    \n",
    "    return levenshtein_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab['levenshtein_dist'] = add_levenshtein_distance(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use dill - works the same way as pickle\n",
    "import dill\n",
    "with open('data/processed/vocab.pkl', 'wb') as file:\n",
    "    dill.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def assert_vocab_remove_articles():\n",
    "\n",
    "    # Creating test dataframe\n",
    "    vocab_test = pd.DataFrame(columns=['german', 'english'])\n",
    "    vocab_test = vocab_test.append({'german': 'dienstag', 'english': 'tuesday'}, ignore_index=True)\n",
    "    vocab_test = vocab_test.append({'german': 'studieren', 'english': 'to study'}, ignore_index=True)\n",
    "    vocab_test = vocab_test.append({'german': 'die angst', 'english': 'the fear'}, ignore_index=True)\n",
    "    vocab_test = vocab_test.append({'german': 'andere', 'english': 'other'}, ignore_index=True)\n",
    "    vocab_test = vocab_test.append({'german': 'der arzt', 'english': 'the doctor'}, ignore_index=True)\n",
    "    vocab_test = vocab_test.append({'german': 'das café', 'english': 'the café'}, ignore_index=True)\n",
    "\n",
    "    # Creating test dataframe\n",
    "    vocab_results = pd.DataFrame(columns=['german', 'english'])\n",
    "    vocab_results = vocab_results.append({'german': 'dienstag', 'english': 'tuesday'}, ignore_index=True)\n",
    "    vocab_results = vocab_results.append({'german': 'studieren', 'english': 'study'}, ignore_index=True)\n",
    "    vocab_results = vocab_results.append({'german': 'angst', 'english': 'fear'}, ignore_index=True)\n",
    "    vocab_results = vocab_results.append({'german': 'andere', 'english': 'other'}, ignore_index=True)\n",
    "    vocab_results = vocab_results.append({'german': 'arzt', 'english': 'doctor'}, ignore_index=True)\n",
    "    vocab_results = vocab_results.append({'german': 'café', 'english': 'café'}, ignore_index=True)\n",
    "\n",
    "    return vocab_test, vocab_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert_vocab_remove_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Length words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab[\"nb_characters_german\"] = vocab[\"german\"].map(len)\n",
    "vocab[\"nb_characters_english\"] = vocab[\"english\"].map(len)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_nb_words_german(x):\n",
    "    list_german_article = [\"der\", \"die\", \"das\"]\n",
    "    separate_words = x.split(\" \")\n",
    "    if separate_words[0] in list_german_article:\n",
    "        separate_words = separate_words[1:]\n",
    "    return len(separate_words)\n",
    "\n",
    "\n",
    "def count_nb_words_english(x):\n",
    "    list_english_article = [\"the\", \"to\"]\n",
    "    separate_words = x.split(\" \")\n",
    "    if separate_words[0] in list_english_article:\n",
    "        separate_words = separate_words[1:]\n",
    "    return len(separate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab[\"nb_words_german\"] = vocab[\"german\"].map(count_nb_words_german)\n",
    "vocab[\"nb_words_english\"] = vocab[\"english\"].map(count_nb_words_english)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab[\"nb_words_english\"] = vocab[\"english\"].map(lambda x: len(x.split(\" \")))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_noun(x, list_german_article=[\"der\", \"die\", \"das\"]):\n",
    "    possible_article = x[\"german\"].split(\" \", 1)[0]\n",
    "    return possible_article in list_german_article\n",
    "\n",
    "\n",
    "vocab[\"is_noun\"] = vocab.apply(is_noun, axis=1)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def is_verb(x):\n",
    "    possible_article = x[\"english\"].split(\" \", 1)[0]\n",
    "    return \"to\" in possible_article\n",
    "\n",
    "\n",
    "vocab[\"is_verb\"] = vocab.apply(is_verb, axis=1)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word difficulty category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_difficulty_category(vocab):\n",
    "\n",
    "    dict_difficulty_category = {\n",
    "        \"Minus10points\": -10,\n",
    "        \"Minus9points\": -9,\n",
    "        \"Minus8points\": -8,\n",
    "        \"Minus7points\": -7,\n",
    "        \"Minus6points\": -6,\n",
    "        \"Minus5points\": -5,\n",
    "        \"Minus4points\": -4,\n",
    "        \"Minus3points\": -3,\n",
    "        \"Minus2points\": -2,\n",
    "        \"Minus1points\": -1,\n",
    "        \"0points\": 0,\n",
    "        \"1points\": 1,\n",
    "        \"2points\": 2,\n",
    "        \"3points\": 3,\n",
    "        \"4points\": 4,\n",
    "        \"5points\": 5,\n",
    "    }\n",
    "\n",
    "    original_vocab = pd.DataFrame()\n",
    "\n",
    "    for difficulty_category in dict_difficulty_category.keys():\n",
    "\n",
    "        i_original_vocab = pd.read_csv(\n",
    "            f\"data/raw/new_vocabulary/{difficulty_category}.csv\"\n",
    "        )\n",
    "        i_original_vocab[\"difficulty_category\"] = dict_difficulty_category[\n",
    "            difficulty_category\n",
    "        ]\n",
    "        original_vocab = original_vocab.append(i_original_vocab)\n",
    "\n",
    "    vocab = pd.merge(\n",
    "        vocab,\n",
    "        original_vocab[[\"German\", \"English\", \"difficulty_category\"]],\n",
    "        left_on=[\"german\", \"english\"],\n",
    "        right_on=[\"German\", \"English\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    del vocab[\"German\"]\n",
    "    del vocab[\"English\"]\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_difficulty_category(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "vocabulary_learning",
   "language": "python",
   "name": "vocabulary_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
