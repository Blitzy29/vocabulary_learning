{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "def update_working_directory():\n",
    "    from pathlib import Path\n",
    "\n",
    "    p = Path(os.getcwd()).parents[0]\n",
    "    os.chdir(p)\n",
    "    print(p)\n",
    "\n",
    "\n",
    "update_working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill\n",
    "import luigi\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "import src.data.get_dataset as get_dataset\n",
    "import src.data.make_dataset as make_dataset\n",
    "from src.data.make_historical_features import create_historical_features\n",
    "from src.data.make_vocab_features import create_vocab_features\n",
    "\n",
    "from src.models.logistic_regression import ModelLogisticRegression\n",
    "\n",
    "from src.data.make_predictions_next_session import (\n",
    "    make_and_save_predictions_next_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: create a new folder where the pipeline will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatePipelineFolder(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create pipeline folder\")\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/01_create_pipeline_folder.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        newpath = r\"data/pipeline/{}\".format(today)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Pipeline folder created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: copy the historical data and vocab data from `official` to pipeline folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyFilesIntoPipeline(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"copy files into pipeline\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CreatePipelineFolder()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/02_copy_files_into_pipeline.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        for i_file in [\"german_english.csv\", \"historical_data.csv\"]:\n",
    "            copyfile(\n",
    "                src=r\"data/official/{}\".format(i_file),\n",
    "                dst=r\"data/pipeline/{}/{}\".format(today, i_file),\n",
    "            )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Files into pipeline copied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3A: create historical dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateHistoricalDatasetFeatures(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create historical dataset features.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyFilesIntoPipeline()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/03A_create_historical_dataset_features.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        historical_data = get_dataset.get_historical_data(\n",
    "            historical_data_path=\"data/pipeline/{}/{}\".format(\n",
    "                today, \"historical_data.csv\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        historical_data = create_historical_features(historical_data)\n",
    "\n",
    "        historical_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"historical_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(historical_data_features_path, \"wb\") as file:\n",
    "            dill.dump(historical_data, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Historical dataset features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3B: create vocab dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateVocabDatasetFeatures(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create vocab dataset features.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyFilesIntoPipeline()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/03B_create_vocab_dataset_features.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        vocab = get_dataset.get_vocab(\n",
    "            vocab_path=\"data/pipeline/{}/{}\".format(today, \"german_english.csv\"),\n",
    "            list_columns=\"all\",\n",
    "        )\n",
    "        vocab = create_vocab_features(vocab)\n",
    "\n",
    "        vocab_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"vocab_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(vocab_data_features_path, \"wb\") as file:\n",
    "            dill.dump(vocab, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Vocab dataset features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: merge features dataset together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeFeaturesTogether(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(\n",
    "        default=\"merge historical and vocab dataset features together.\"\n",
    "    )\n",
    "\n",
    "    def requires(self):\n",
    "        return CreateHistoricalDatasetFeatures(), CreateVocabDatasetFeatures()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/04_merge_features_together.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        historical_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"historical_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(historical_data_features_path, \"rb\") as input_file:\n",
    "            historical_data_features = dill.load(input_file)\n",
    "\n",
    "        vocab_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"vocab_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(vocab_data_features_path, \"rb\") as input_file:\n",
    "            vocab_data_features = dill.load(input_file)\n",
    "\n",
    "        dataset = make_dataset.merge_feature_datasets(\n",
    "            historical_data_features, vocab_data_features\n",
    "        )\n",
    "\n",
    "        vardict = make_dataset.get_vardict()\n",
    "        dataset = make_dataset.transform_type(dataset, vardict)\n",
    "\n",
    "        dataset_path = \"data/pipeline/{}/{}\".format(today, \"dataset.pkl\")\n",
    "\n",
    "        with open(dataset_path, \"wb\") as file:\n",
    "            dill.dump(dataset, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Historical and Vocab dataset features merged together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: split into train/validation/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitDatasetIntoTrainValidTest(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"split the dataset into train/valid/test datasets.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return MergeFeaturesTogether()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/05_split_dataset_into_train_valid_test.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        dataset_path = \"data/pipeline/{}/{}\".format(today, \"dataset.pkl\")\n",
    "        with open(dataset_path, \"rb\") as input_file:\n",
    "            dataset = dill.load(input_file)\n",
    "\n",
    "        train_dataset_path = \"data/pipeline/{}/{}\".format(today, \"train_dataset.pkl\")\n",
    "        valid_dataset_path = \"data/pipeline/{}/{}\".format(today, \"valid_dataset.pkl\")\n",
    "        test_dataset_path = \"data/pipeline/{}/{}\".format(today, \"test_dataset.pkl\")\n",
    "\n",
    "        make_dataset.split_train_valid_test_dataset(\n",
    "            dataset, train_dataset_path, valid_dataset_path, test_dataset_path\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Train, validation and test datasets splitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogisticRegressionModel(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"train logistic regression model\")\n",
    "\n",
    "    def requires(self):\n",
    "        return SplitDatasetIntoTrainValidTest()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/06_train_logistic_regression_model.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        path_dataset_train = \"data/pipeline/{}/{}\".format(today, \"train_dataset.pkl\")\n",
    "        with open(path_dataset_train, \"rb\") as input_file:\n",
    "            dataset_train = dill.load(input_file)\n",
    "\n",
    "        model = ModelLogisticRegression()\n",
    "        dataset_train = model.preprocessing_training(dataset_train)\n",
    "        model.train(dataset_train)\n",
    "        model.plot_coefficients()\n",
    "\n",
    "        path_dataset_valid = \"data/pipeline/{}/{}\".format(today, \"valid_dataset.pkl\")\n",
    "        with open(path_dataset_valid, \"rb\") as input_file:\n",
    "            dataset_valid = dill.load(input_file)\n",
    "\n",
    "        model.predict_and_show_results(dataset_valid, save_folder=\"data/pipeline/{}\".format(today))\n",
    "\n",
    "        path_model = \"data/pipeline/{}/{}\".format(today, \"model.pkl\")\n",
    "        with open(path_model, \"wb\") as file:\n",
    "            dill.dump(model, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: create next session historical and vocab features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateNewSessionFeaturesDataset(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create new session dataset features.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyFilesIntoPipeline()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/07_create_new_session_dataset_features.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        historical_data = get_dataset.get_historical_data(\n",
    "            historical_data_path=\"data/pipeline/{}/{}\".format(\n",
    "                today, \"historical_data.csv\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        vocab = get_dataset.get_vocab(\n",
    "            vocab_path=\"data/pipeline/{}/{}\".format(today, \"german_english.csv\"),\n",
    "            list_columns=\"all\",\n",
    "        )\n",
    "\n",
    "        dataset_predictions_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"new_session_features_dataset.pkl\"\n",
    "        )\n",
    "        make_dataset.create_dataset_new_session(\n",
    "            dataset_predictions_path,\n",
    "            historical_data=historical_data,\n",
    "            vocab_to_predict=vocab,\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"New session features dataset created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakePredictions(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"make predictions\")\n",
    "\n",
    "    def requires(self):\n",
    "        return TrainLogisticRegressionModel(), CreateNewSessionFeaturesDataset()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/08_make_predictions.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        path_model = \"data/pipeline/{}/{}\".format(today, \"model.pkl\")\n",
    "        with open(path_model, \"rb\") as input_file:\n",
    "            model = dill.load(input_file)\n",
    "\n",
    "        next_session_features_dataset_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"new_session_features_dataset.pkl\"\n",
    "        )\n",
    "        with open(next_session_features_dataset_path, \"rb\") as input_file:\n",
    "            next_session_features_dataset = dill.load(input_file)\n",
    "\n",
    "        next_session_probas_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"next_session_probas_path.pkl\"\n",
    "        )\n",
    "\n",
    "        make_and_save_predictions_next_session(\n",
    "            model=model,\n",
    "            next_session_features_dataset=next_session_features_dataset,\n",
    "            probas_next_session_path=next_session_probas_path,\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Predictions made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: copy predictions to csv and official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyPredictionsToOfficial(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"copy predictions to 'official' as a csv\")\n",
    "\n",
    "    def requires(self):\n",
    "        return MakePredictions()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/09_copy_predictions_to_official.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        next_session_probas_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"next_session_probas_path.pkl\"\n",
    "        )\n",
    "        with open(next_session_probas_path, \"rb\") as input_file:\n",
    "            next_session_probas = dill.load(input_file)\n",
    "\n",
    "        predictions_next_session_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"predictions_next_session.csv\"\n",
    "        )\n",
    "        next_session_probas.to_csv(predictions_next_session_path, index=False)\n",
    "\n",
    "        copyfile(\n",
    "            src=r\"data/pipeline/{}/{}\".format(today, \"predictions_next_session.csv\"),\n",
    "            dst=r\"data/official/{}\".format(\"predictions_next_session.csv\"),\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Predictions copied in csv format to official.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10: Do everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareNextSession(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"merge historical and vocab dataset features together.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyPredictionsToOfficial()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/10_prepare_next_session.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "            \n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Next session ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "vocabulary_learning",
   "language": "python",
   "name": "vocabulary_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
