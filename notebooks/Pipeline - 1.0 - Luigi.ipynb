{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:11.836673Z",
     "start_time": "2020-10-23T18:19:11.677703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Projects/vocabulary_learning/notebooks\n",
      "/Users/admin/Projects/vocabulary_learning\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n\\nimport os\\n\\nprint(os.getcwd())\\n\\n\\ndef update_working_directory():\\n    from pathlib import Path\\n\\n    p = Path(os.getcwd()).parents[0]\\n    os.chdir(p)\\n    print(p)\\n\\n\\nupdate_working_directory()\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n\\nimport os\\n\\nprint(os.getcwd())\\n\\n\\ndef update_working_directory():\\n    from pathlib import Path\\n\\n    p = Path(os.getcwd()).parents[0]\\n    os.chdir(p)\\n    print(p)\\n\\n\\nupdate_working_directory()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "def update_working_directory():\n",
    "    from pathlib import Path\n",
    "\n",
    "    p = Path(os.getcwd()).parents[0]\n",
    "    os.chdir(p)\n",
    "    print(p)\n",
    "\n",
    "\n",
    "update_working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.579131Z",
     "start_time": "2020-10-23T18:19:11.839538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import datetime\\nimport dill\\nimport luigi\\nimport os\\nfrom shutil import copyfile\\n\\nimport src.data.get_dataset as get_dataset\\nimport src.data.make_dataset as make_dataset\\nfrom src.data.make_historical_features import create_historical_features\\nfrom src.data.make_vocab_features import create_vocab_features\\n\\nfrom src.models.logistic_regression import ModelLogisticRegression\\n\\nfrom src.data.make_predictions_next_session import (\\n    make_and_save_predictions_next_session,\\n)\";\n",
       "                var nbb_formatted_code = \"import datetime\\nimport dill\\nimport luigi\\nimport os\\nfrom shutil import copyfile\\n\\nimport src.data.get_dataset as get_dataset\\nimport src.data.make_dataset as make_dataset\\nfrom src.data.make_historical_features import create_historical_features\\nfrom src.data.make_vocab_features import create_vocab_features\\n\\nfrom src.models.logistic_regression import ModelLogisticRegression\\n\\nfrom src.data.make_predictions_next_session import (\\n    make_and_save_predictions_next_session,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import dill\n",
    "import luigi\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "import src.data.get_dataset as get_dataset\n",
    "import src.data.make_dataset as make_dataset\n",
    "from src.data.make_historical_features import create_historical_features\n",
    "from src.data.make_vocab_features import create_vocab_features\n",
    "\n",
    "from src.models.logistic_regression import ModelLogisticRegression\n",
    "\n",
    "from src.data.make_predictions_next_session import (\n",
    "    make_and_save_predictions_next_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.617287Z",
     "start_time": "2020-10-23T18:19:13.582456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"today = datetime.datetime.today().strftime(\\\"%Y%m%d\\\")\";\n",
       "                var nbb_formatted_code = \"today = datetime.datetime.today().strftime(\\\"%Y%m%d\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "today = datetime.datetime.today().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: create a new folder where the pipeline will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.666066Z",
     "start_time": "2020-10-23T18:19:13.619953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class CreatePipelineFolder(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create pipeline folder\\\")\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\\"data/pipeline/{}/01_create_pipeline_folder.txt\\\".format(today))\\n\\n    def run(self):\\n        newpath = r\\\"data/pipeline/{}\\\".format(today)\\n        if not os.path.exists(newpath):\\n            os.makedirs(newpath)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Pipeline folder created.\\\")\";\n",
       "                var nbb_formatted_code = \"class CreatePipelineFolder(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create pipeline folder\\\")\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/01_create_pipeline_folder.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n        newpath = r\\\"data/pipeline/{}\\\".format(today)\\n        if not os.path.exists(newpath):\\n            os.makedirs(newpath)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Pipeline folder created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CreatePipelineFolder(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create pipeline folder\")\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/01_create_pipeline_folder.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        newpath = r\"data/pipeline/{}\".format(today)\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Pipeline folder created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: copy the historical data and vocab data from `official` to pipeline folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.718531Z",
     "start_time": "2020-10-23T18:19:13.669120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"class CopyFilesIntoPipeline(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"copy files into pipeline\\\")\\n\\n    def requires(self):\\n        return CreatePipelineFolder()\\n    \\n    def output(self):\\n        return luigi.LocalTarget(\\\"data/pipeline/{}/02_copy_files_into_pipeline.txt\\\".format(today))\\n    \\n    def run(self):    \\n        for i_file in [\\\"german_english.csv\\\", \\\"historical_data.csv\\\"]:\\n            copyfile(\\n                src=r\\\"data/official/{}\\\".format(i_file),\\n                dst=r\\\"data/pipeline/{}/{}\\\".format(today, i_file),\\n            )\\n            \\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Files into pipeline copied.\\\")\";\n",
       "                var nbb_formatted_code = \"class CopyFilesIntoPipeline(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"copy files into pipeline\\\")\\n\\n    def requires(self):\\n        return CreatePipelineFolder()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/02_copy_files_into_pipeline.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n        for i_file in [\\\"german_english.csv\\\", \\\"historical_data.csv\\\"]:\\n            copyfile(\\n                src=r\\\"data/official/{}\\\".format(i_file),\\n                dst=r\\\"data/pipeline/{}/{}\\\".format(today, i_file),\\n            )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Files into pipeline copied.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CopyFilesIntoPipeline(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"copy files into pipeline\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CreatePipelineFolder()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/02_copy_files_into_pipeline.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        for i_file in [\"german_english.csv\", \"historical_data.csv\"]:\n",
    "            copyfile(\n",
    "                src=r\"data/official/{}\".format(i_file),\n",
    "                dst=r\"data/pipeline/{}/{}\".format(today, i_file),\n",
    "            )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Files into pipeline copied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3A: create historical dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.780393Z",
     "start_time": "2020-10-23T18:19:13.721182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"class CreateHistoricalDatasetFeatures(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create historical dataset features.\\\")\\n\\n    def requires(self):\\n        return CopyFilesIntoPipeline()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/03A_create_historical_dataset_features.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        historical_data = get_dataset.get_historical_data(\\n            historical_data_path=\\\"data/pipeline/{}/{}\\\".format(\\n                today, \\\"historical_data.csv\\\"\\n            ),\\n        )\\n\\n        historical_data = create_historical_features(historical_data)\\n\\n        historical_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"historical_dataset_features.pkl\\\"\\n        )\\n        with open(historical_data_features_path, \\\"wb\\\") as file:\\n            dill.dump(historical_data, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Historical dataset features created.\\\")\";\n",
       "                var nbb_formatted_code = \"class CreateHistoricalDatasetFeatures(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create historical dataset features.\\\")\\n\\n    def requires(self):\\n        return CopyFilesIntoPipeline()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/03A_create_historical_dataset_features.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        historical_data = get_dataset.get_historical_data(\\n            historical_data_path=\\\"data/pipeline/{}/{}\\\".format(\\n                today, \\\"historical_data.csv\\\"\\n            ),\\n        )\\n\\n        historical_data = create_historical_features(historical_data)\\n\\n        historical_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"historical_dataset_features.pkl\\\"\\n        )\\n        with open(historical_data_features_path, \\\"wb\\\") as file:\\n            dill.dump(historical_data, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Historical dataset features created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CreateHistoricalDatasetFeatures(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create historical dataset features.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyFilesIntoPipeline()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/03A_create_historical_dataset_features.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        historical_data = get_dataset.get_historical_data(\n",
    "            historical_data_path=\"data/pipeline/{}/{}\".format(\n",
    "                today, \"historical_data.csv\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        historical_data = create_historical_features(historical_data)\n",
    "\n",
    "        historical_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"historical_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(historical_data_features_path, \"wb\") as file:\n",
    "            dill.dump(historical_data, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Historical dataset features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3B: create vocab dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.844276Z",
     "start_time": "2020-10-23T18:19:13.783762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"class CreateVocabDatasetFeatures(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create vocab dataset features.\\\")\\n\\n    def requires(self):\\n        return CopyFilesIntoPipeline()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/03B_create_vocab_dataset_features.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        vocab = get_dataset.get_vocab(\\n            vocab_path=\\\"data/pipeline/{}/{}\\\".format(today, \\\"german_english.csv\\\"), list_columns=\\\"all\\\"\\n        )\\n        vocab = create_vocab_features(vocab)\\n\\n        vocab_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"vocab_dataset_features.pkl\\\"\\n        )\\n        with open(vocab_data_features_path, \\\"wb\\\") as file:\\n            dill.dump(vocab, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Vocab dataset features created.\\\")\";\n",
       "                var nbb_formatted_code = \"class CreateVocabDatasetFeatures(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create vocab dataset features.\\\")\\n\\n    def requires(self):\\n        return CopyFilesIntoPipeline()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/03B_create_vocab_dataset_features.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        vocab = get_dataset.get_vocab(\\n            vocab_path=\\\"data/pipeline/{}/{}\\\".format(today, \\\"german_english.csv\\\"),\\n            list_columns=\\\"all\\\",\\n        )\\n        vocab = create_vocab_features(vocab)\\n\\n        vocab_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"vocab_dataset_features.pkl\\\"\\n        )\\n        with open(vocab_data_features_path, \\\"wb\\\") as file:\\n            dill.dump(vocab, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Vocab dataset features created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CreateVocabDatasetFeatures(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create vocab dataset features.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyFilesIntoPipeline()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/03B_create_vocab_dataset_features.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        vocab = get_dataset.get_vocab(\n",
    "            vocab_path=\"data/pipeline/{}/{}\".format(today, \"german_english.csv\"),\n",
    "            list_columns=\"all\",\n",
    "        )\n",
    "        vocab = create_vocab_features(vocab)\n",
    "\n",
    "        vocab_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"vocab_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(vocab_data_features_path, \"wb\") as file:\n",
    "            dill.dump(vocab, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Vocab dataset features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: merge features dataset together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.911209Z",
     "start_time": "2020-10-23T18:19:13.848495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"class MergeFeaturesTogether(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"merge historical and vocab dataset features together.\\\")\\n\\n    def requires(self):\\n        return CreateHistoricalDatasetFeatures(), CreateVocabDatasetFeatures()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/04_merge_features_together.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n        \\n        historical_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"historical_dataset_features.pkl\\\"\\n        )\\n        with open(historical_data_features_path, \\\"rb\\\") as input_file:\\n            historical_data_features = dill.load(input_file)\\n\\n        vocab_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"vocab_dataset_features.pkl\\\"\\n        )\\n        with open(vocab_data_features_path, \\\"rb\\\") as input_file:\\n            vocab_data_features = dill.load(input_file)\\n\\n        dataset = make_dataset.merge_feature_datasets(\\n            historical_data_features, vocab_data_features\\n        )\\n\\n        vardict = make_dataset.get_vardict()\\n        dataset = make_dataset.transform_type(dataset, vardict)\\n\\n        dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"dataset.pkl\\\")\\n\\n        with open(dataset_path, \\\"wb\\\") as file:\\n            dill.dump(dataset, file)\\n            \\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Historical and Vocab dataset features merged together.\\\")\";\n",
       "                var nbb_formatted_code = \"class MergeFeaturesTogether(luigi.Task):\\n\\n    name = luigi.Parameter(\\n        default=\\\"merge historical and vocab dataset features together.\\\"\\n    )\\n\\n    def requires(self):\\n        return CreateHistoricalDatasetFeatures(), CreateVocabDatasetFeatures()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/04_merge_features_together.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        historical_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"historical_dataset_features.pkl\\\"\\n        )\\n        with open(historical_data_features_path, \\\"rb\\\") as input_file:\\n            historical_data_features = dill.load(input_file)\\n\\n        vocab_data_features_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"vocab_dataset_features.pkl\\\"\\n        )\\n        with open(vocab_data_features_path, \\\"rb\\\") as input_file:\\n            vocab_data_features = dill.load(input_file)\\n\\n        dataset = make_dataset.merge_feature_datasets(\\n            historical_data_features, vocab_data_features\\n        )\\n\\n        vardict = make_dataset.get_vardict()\\n        dataset = make_dataset.transform_type(dataset, vardict)\\n\\n        dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"dataset.pkl\\\")\\n\\n        with open(dataset_path, \\\"wb\\\") as file:\\n            dill.dump(dataset, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Historical and Vocab dataset features merged together.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MergeFeaturesTogether(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(\n",
    "        default=\"merge historical and vocab dataset features together.\"\n",
    "    )\n",
    "\n",
    "    def requires(self):\n",
    "        return CreateHistoricalDatasetFeatures(), CreateVocabDatasetFeatures()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/04_merge_features_together.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        historical_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"historical_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(historical_data_features_path, \"rb\") as input_file:\n",
    "            historical_data_features = dill.load(input_file)\n",
    "\n",
    "        vocab_data_features_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"vocab_dataset_features.pkl\"\n",
    "        )\n",
    "        with open(vocab_data_features_path, \"rb\") as input_file:\n",
    "            vocab_data_features = dill.load(input_file)\n",
    "\n",
    "        dataset = make_dataset.merge_feature_datasets(\n",
    "            historical_data_features, vocab_data_features\n",
    "        )\n",
    "\n",
    "        vardict = make_dataset.get_vardict()\n",
    "        dataset = make_dataset.transform_type(dataset, vardict)\n",
    "\n",
    "        dataset_path = \"data/pipeline/{}/{}\".format(today, \"dataset.pkl\")\n",
    "\n",
    "        with open(dataset_path, \"wb\") as file:\n",
    "            dill.dump(dataset, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Historical and Vocab dataset features merged together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: split into train/validation/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:13.972117Z",
     "start_time": "2020-10-23T18:19:13.914416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"class SplitDatasetIntoTrainValidTest(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"split the dataset into train/valid/test datasets.\\\")\\n\\n    def requires(self):\\n        return MergeFeaturesTogether()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/05_split_dataset_into_train_valid_test.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"dataset.pkl\\\")\\n        with open(dataset_path, \\\"rb\\\") as input_file:\\n            dataset = dill.load(input_file)\\n\\n        train_dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"train_dataset.pkl\\\")\\n        valid_dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"valid_dataset.pkl\\\")\\n        test_dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"test_dataset.pkl\\\")\\n\\n        make_dataset.split_train_valid_test_dataset(\\n            dataset, train_dataset_path, valid_dataset_path, test_dataset_path\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Train, validation and test datasets splitted.\\\")\";\n",
       "                var nbb_formatted_code = \"class SplitDatasetIntoTrainValidTest(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"split the dataset into train/valid/test datasets.\\\")\\n\\n    def requires(self):\\n        return MergeFeaturesTogether()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/05_split_dataset_into_train_valid_test.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"dataset.pkl\\\")\\n        with open(dataset_path, \\\"rb\\\") as input_file:\\n            dataset = dill.load(input_file)\\n\\n        train_dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"train_dataset.pkl\\\")\\n        valid_dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"valid_dataset.pkl\\\")\\n        test_dataset_path = \\\"data/pipeline/{}/{}\\\".format(today, \\\"test_dataset.pkl\\\")\\n\\n        make_dataset.split_train_valid_test_dataset(\\n            dataset, train_dataset_path, valid_dataset_path, test_dataset_path\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Train, validation and test datasets splitted.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SplitDatasetIntoTrainValidTest(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"split the dataset into train/valid/test datasets.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return MergeFeaturesTogether()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/05_split_dataset_into_train_valid_test.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        dataset_path = \"data/pipeline/{}/{}\".format(today, \"dataset.pkl\")\n",
    "        with open(dataset_path, \"rb\") as input_file:\n",
    "            dataset = dill.load(input_file)\n",
    "\n",
    "        train_dataset_path = \"data/pipeline/{}/{}\".format(today, \"train_dataset.pkl\")\n",
    "        valid_dataset_path = \"data/pipeline/{}/{}\".format(today, \"valid_dataset.pkl\")\n",
    "        test_dataset_path = \"data/pipeline/{}/{}\".format(today, \"test_dataset.pkl\")\n",
    "\n",
    "        make_dataset.split_train_valid_test_dataset(\n",
    "            dataset, train_dataset_path, valid_dataset_path, test_dataset_path\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Train, validation and test datasets splitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:14.026555Z",
     "start_time": "2020-10-23T18:19:13.974270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"class TrainLogisticRegressionModel(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"train logistic regression model\\\")\\n\\n    def requires(self):\\n        return SplitDatasetIntoTrainValidTest()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/06_train_logistic_regression_model.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        path_dataset_train = \\\"data/pipeline/{}/{}\\\".format(today, \\\"train_dataset.pkl\\\")\\n        with open(path_dataset_train, \\\"rb\\\") as input_file:\\n            dataset_train = dill.load(input_file)\\n\\n        model = ModelLogisticRegression()\\n        dataset_train = model.preprocessing_training(dataset_train)\\n        model.train(dataset_train)\\n\\n        path_model = \\\"data/pipeline/{}/{}\\\".format(today, \\\"model.pkl\\\")\\n        with open(path_model, \\\"wb\\\") as file:\\n            dill.dump(model, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Model trained.\\\")\";\n",
       "                var nbb_formatted_code = \"class TrainLogisticRegressionModel(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"train logistic regression model\\\")\\n\\n    def requires(self):\\n        return SplitDatasetIntoTrainValidTest()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/06_train_logistic_regression_model.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        path_dataset_train = \\\"data/pipeline/{}/{}\\\".format(today, \\\"train_dataset.pkl\\\")\\n        with open(path_dataset_train, \\\"rb\\\") as input_file:\\n            dataset_train = dill.load(input_file)\\n\\n        model = ModelLogisticRegression()\\n        dataset_train = model.preprocessing_training(dataset_train)\\n        model.train(dataset_train)\\n\\n        path_model = \\\"data/pipeline/{}/{}\\\".format(today, \\\"model.pkl\\\")\\n        with open(path_model, \\\"wb\\\") as file:\\n            dill.dump(model, file)\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Model trained.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TrainLogisticRegressionModel(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"train logistic regression model\")\n",
    "\n",
    "    def requires(self):\n",
    "        return SplitDatasetIntoTrainValidTest()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/06_train_logistic_regression_model.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        path_dataset_train = \"data/pipeline/{}/{}\".format(today, \"train_dataset.pkl\")\n",
    "        with open(path_dataset_train, \"rb\") as input_file:\n",
    "            dataset_train = dill.load(input_file)\n",
    "\n",
    "        model = ModelLogisticRegression()\n",
    "        dataset_train = model.preprocessing_training(dataset_train)\n",
    "        model.train(dataset_train)\n",
    "\n",
    "        path_model = \"data/pipeline/{}/{}\".format(today, \"model.pkl\")\n",
    "        with open(path_model, \"wb\") as file:\n",
    "            dill.dump(model, file)\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T17:36:38.288433Z",
     "start_time": "2020-10-23T17:36:38.255482Z"
    }
   },
   "source": [
    "# Task 7: create next session historical and vocab features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:14.082279Z",
     "start_time": "2020-10-23T18:19:14.028935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"class CreateNewSessionFeaturesDataset(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create new session dataset features.\\\")\\n\\n    def requires(self):\\n        return CopyFilesIntoPipeline()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/07_create_new_session_dataset_features.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        historical_data = get_dataset.get_historical_data(\\n            historical_data_path=\\\"data/pipeline/{}/{}\\\".format(\\n                today, \\\"historical_data.csv\\\"\\n            ),\\n        )\\n\\n        vocab = get_dataset.get_vocab(\\n            vocab_path=\\\"data/pipeline/{}/{}\\\".format(today, \\\"german_english.csv\\\"),\\n            list_columns=\\\"all\\\",\\n        )\\n\\n        dataset_predictions_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"new_session_features_dataset.pkl\\\"\\n        )\\n        make_dataset.create_dataset_new_session(\\n            dataset_predictions_path,\\n            historical_data=historical_data,\\n            vocab_to_predict=vocab,\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"New session features dataset created.\\\")\";\n",
       "                var nbb_formatted_code = \"class CreateNewSessionFeaturesDataset(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"create new session dataset features.\\\")\\n\\n    def requires(self):\\n        return CopyFilesIntoPipeline()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/07_create_new_session_dataset_features.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        historical_data = get_dataset.get_historical_data(\\n            historical_data_path=\\\"data/pipeline/{}/{}\\\".format(\\n                today, \\\"historical_data.csv\\\"\\n            ),\\n        )\\n\\n        vocab = get_dataset.get_vocab(\\n            vocab_path=\\\"data/pipeline/{}/{}\\\".format(today, \\\"german_english.csv\\\"),\\n            list_columns=\\\"all\\\",\\n        )\\n\\n        dataset_predictions_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"new_session_features_dataset.pkl\\\"\\n        )\\n        make_dataset.create_dataset_new_session(\\n            dataset_predictions_path,\\n            historical_data=historical_data,\\n            vocab_to_predict=vocab,\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"New session features dataset created.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CreateNewSessionFeaturesDataset(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"create new session dataset features.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyFilesIntoPipeline()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/07_create_new_session_dataset_features.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        historical_data = get_dataset.get_historical_data(\n",
    "            historical_data_path=\"data/pipeline/{}/{}\".format(\n",
    "                today, \"historical_data.csv\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        vocab = get_dataset.get_vocab(\n",
    "            vocab_path=\"data/pipeline/{}/{}\".format(today, \"german_english.csv\"),\n",
    "            list_columns=\"all\",\n",
    "        )\n",
    "\n",
    "        dataset_predictions_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"new_session_features_dataset.pkl\"\n",
    "        )\n",
    "        make_dataset.create_dataset_new_session(\n",
    "            dataset_predictions_path,\n",
    "            historical_data=historical_data,\n",
    "            vocab_to_predict=vocab,\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"New session features dataset created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:14.146279Z",
     "start_time": "2020-10-23T18:19:14.084481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"class MakePredictions(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"make predictions\\\")\\n\\n    def requires(self):\\n        return TrainLogisticRegressionModel(), CreateNewSessionFeaturesDataset()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/08_make_predictions.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        path_model = \\\"data/pipeline/{}/{}\\\".format(today, \\\"model.pkl\\\")\\n        with open(path_model, \\\"rb\\\") as input_file:\\n            model = dill.load(input_file)\\n\\n        next_session_features_dataset_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"new_session_features_dataset.pkl\\\"\\n        )\\n        with open(next_session_features_dataset_path, \\\"rb\\\") as input_file:\\n            next_session_features_dataset = dill.load(input_file)\\n\\n        next_session_probas_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"next_session_probas_path.pkl\\\"\\n        )\\n\\n        make_and_save_predictions_next_session(\\n            model=model,\\n            next_session_features_dataset=next_session_features_dataset,\\n            probas_next_session_path=next_session_probas_path,\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Predictions made.\\\")\";\n",
       "                var nbb_formatted_code = \"class MakePredictions(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"make predictions\\\")\\n\\n    def requires(self):\\n        return TrainLogisticRegressionModel(), CreateNewSessionFeaturesDataset()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/08_make_predictions.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        path_model = \\\"data/pipeline/{}/{}\\\".format(today, \\\"model.pkl\\\")\\n        with open(path_model, \\\"rb\\\") as input_file:\\n            model = dill.load(input_file)\\n\\n        next_session_features_dataset_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"new_session_features_dataset.pkl\\\"\\n        )\\n        with open(next_session_features_dataset_path, \\\"rb\\\") as input_file:\\n            next_session_features_dataset = dill.load(input_file)\\n\\n        next_session_probas_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"next_session_probas_path.pkl\\\"\\n        )\\n\\n        make_and_save_predictions_next_session(\\n            model=model,\\n            next_session_features_dataset=next_session_features_dataset,\\n            probas_next_session_path=next_session_probas_path,\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Predictions made.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MakePredictions(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"make predictions\")\n",
    "\n",
    "    def requires(self):\n",
    "        return TrainLogisticRegressionModel(), CreateNewSessionFeaturesDataset()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/08_make_predictions.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        path_model = \"data/pipeline/{}/{}\".format(today, \"model.pkl\")\n",
    "        with open(path_model, \"rb\") as input_file:\n",
    "            model = dill.load(input_file)\n",
    "\n",
    "        next_session_features_dataset_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"new_session_features_dataset.pkl\"\n",
    "        )\n",
    "        with open(next_session_features_dataset_path, \"rb\") as input_file:\n",
    "            next_session_features_dataset = dill.load(input_file)\n",
    "\n",
    "        next_session_probas_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"next_session_probas_path.pkl\"\n",
    "        )\n",
    "\n",
    "        make_and_save_predictions_next_session(\n",
    "            model=model,\n",
    "            next_session_features_dataset=next_session_features_dataset,\n",
    "            probas_next_session_path=next_session_probas_path,\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Predictions made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9: copy predictions to csv and official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T18:19:14.209402Z",
     "start_time": "2020-10-23T18:19:14.148603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"class CopyPredictionsToOfficial(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"copy predictions to 'official' as a csv\\\")\\n\\n    def requires(self):\\n        return MakePredictions()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/09_copy_predictions_to_official.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n        \\n        next_session_probas_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"next_session_probas_path.pkl\\\"\\n        )\\n        with open(next_session_probas_path, \\\"rb\\\") as input_file:\\n            next_session_probas = dill.load(input_file)\\n\\n        predictions_next_session_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"predictions_next_session.csv\\\"\\n        )\\n        next_session_probas.to_csv(predictions_next_session_path, index=False)\\n\\n        copyfile(\\n            src=r\\\"data/pipeline/{}/{}\\\".format(today, \\\"predictions_next_session.csv\\\"),\\n            dst=r\\\"data/official/{}\\\".format(\\\"predictions_next_session.csv\\\"),\\n\\n        )\\n            \\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Predictions copied in csv format to official.\\\")\";\n",
       "                var nbb_formatted_code = \"class CopyPredictionsToOfficial(luigi.Task):\\n\\n    name = luigi.Parameter(default=\\\"copy predictions to 'official' as a csv\\\")\\n\\n    def requires(self):\\n        return MakePredictions()\\n\\n    def output(self):\\n        return luigi.LocalTarget(\\n            \\\"data/pipeline/{}/09_copy_predictions_to_official.txt\\\".format(today)\\n        )\\n\\n    def run(self):\\n\\n        next_session_probas_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"next_session_probas_path.pkl\\\"\\n        )\\n        with open(next_session_probas_path, \\\"rb\\\") as input_file:\\n            next_session_probas = dill.load(input_file)\\n\\n        predictions_next_session_path = \\\"data/pipeline/{}/{}\\\".format(\\n            today, \\\"predictions_next_session.csv\\\"\\n        )\\n        next_session_probas.to_csv(predictions_next_session_path, index=False)\\n\\n        copyfile(\\n            src=r\\\"data/pipeline/{}/{}\\\".format(today, \\\"predictions_next_session.csv\\\"),\\n            dst=r\\\"data/official/{}\\\".format(\\\"predictions_next_session.csv\\\"),\\n        )\\n\\n        with self.output().open(\\\"w\\\") as f:\\n            f.write(\\\"Predictions copied in csv format to official.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CopyPredictionsToOfficial(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"copy predictions to 'official' as a csv\")\n",
    "\n",
    "    def requires(self):\n",
    "        return MakePredictions()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/09_copy_predictions_to_official.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        next_session_probas_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"next_session_probas_path.pkl\"\n",
    "        )\n",
    "        with open(next_session_probas_path, \"rb\") as input_file:\n",
    "            next_session_probas = dill.load(input_file)\n",
    "\n",
    "        predictions_next_session_path = \"data/pipeline/{}/{}\".format(\n",
    "            today, \"predictions_next_session.csv\"\n",
    "        )\n",
    "        next_session_probas.to_csv(predictions_next_session_path, index=False)\n",
    "\n",
    "        copyfile(\n",
    "            src=r\"data/pipeline/{}/{}\".format(today, \"predictions_next_session.csv\"),\n",
    "            dst=r\"data/official/{}\".format(\"predictions_next_session.csv\"),\n",
    "        )\n",
    "\n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Predictions copied in csv format to official.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10: Do everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareNextSession(luigi.Task):\n",
    "\n",
    "    name = luigi.Parameter(default=\"merge historical and vocab dataset features together.\")\n",
    "\n",
    "    def requires(self):\n",
    "        return CopyPredictionsToOfficial()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\n",
    "            \"data/pipeline/{}/10_prepare_next_session.txt\".format(today)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "            \n",
    "        with self.output().open(\"w\") as f:\n",
    "            f.write(\"Next session ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "vocabulary_learning",
   "language": "python",
   "name": "vocabulary_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
