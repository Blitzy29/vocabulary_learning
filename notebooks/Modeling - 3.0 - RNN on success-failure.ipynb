{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "def update_working_directory():\n",
    "    from pathlib import Path\n",
    "\n",
    "    p = Path(os.getcwd()).parents[0]\n",
    "    os.chdir(p)\n",
    "    print(p)\n",
    "\n",
    "\n",
    "update_working_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset_train = \"data/raw/20201105/dataset_train.pkl\"\n",
    "path_dataset_valid = \"data/raw/20201105/dataset_valid.pkl\"\n",
    "path_dataset_valid_historical = \"data/raw/20201105/dataset_valid_historical.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.models.fully_connected_nn import ModelFullyConnectedNN\n",
    "import src.models.performance_metrics as performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_dataset_train, \"rb\") as input_file:\n",
    "    dataset_train = dill.load(input_file)\n",
    "\n",
    "with open(path_dataset_valid, \"rb\") as input_file:\n",
    "    dataset_valid = dill.load(input_file)\n",
    "\n",
    "with open(path_dataset_valid_historical, \"rb\") as input_file:\n",
    "    dataset_valid_historical = dill.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelFullyConnectedNN()\n",
    "model.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = model.preprocessing_training(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = model.preprocessing_inference(\n",
    "    dataset=dataset_valid, dataset_historical=dataset_valid_historical\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataset=dataset_train, dataset_test=dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelFullyConnectedNN()\n",
    "model.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = model.preprocessing_training(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for validation\n",
    "\n",
    "**Objective:** have the latest max_session, fill with None\n",
    "\n",
    "We need to take the whole history, train+valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid = model.preprocessing_inference(\n",
    "    dataset=dataset_valid, dataset_historical=dataset_valid_historical\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_sessions_inference = (\n",
    "    max(dataset_train_valid[\"id_session\"]) + 1\n",
    ")  # because it exists id_session = 0\n",
    "nb_sessions_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i_dataset_vocab = dataset_train_valid[\n",
    "    (dataset_train_valid[\"german_word\"] == \"bald\")\n",
    "    & (dataset_train_valid[\"language_asked\"] == \"german\")\n",
    "]\n",
    "\n",
    "i_dataset_vocab[\n",
    "    [\n",
    "        \"id_vocab\",\n",
    "        \"german_word\",\n",
    "        \"english_word\",\n",
    "        \"language_asked\",\n",
    "        \"result\",\n",
    "        \"id_session\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i_id_session = i_dataset_vocab[\"id_session\"].tolist()\n",
    "i_id_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i_result = i_dataset_vocab[\"result\"].tolist()\n",
    "i_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i_results_session = np.full(shape=nb_sessions_inference, fill_value=None).tolist()\n",
    "i_results_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for r, s in zip(i_result, i_id_session):\n",
    "    i_results_session[s] = r\n",
    "i_results_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = lambda x: x in {None}\n",
    "i_results_session = list(strip(iterable=i_results_session, pred=pred))\n",
    "i_results_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from more_itertools import strip\n",
    "\n",
    "\n",
    "def define_word_language_sessions(i_dataset_vocab, nb_sessions, inference=False):\n",
    "\n",
    "    i_id_session = i_dataset_vocab[\"id_session\"].tolist()\n",
    "    i_result = i_dataset_vocab[\"result\"].tolist()\n",
    "\n",
    "    i_results_session = np.full(\n",
    "        shape=max(nb_sessions, max(i_id_session) + 1) if not inference else nb_sessions,\n",
    "        fill_value=None,\n",
    "    ).tolist()\n",
    "\n",
    "    for r, s in zip(i_result, i_id_session):\n",
    "        i_results_session[s] = r\n",
    "\n",
    "    i_results_session = list(\n",
    "        strip(iterable=i_results_session, pred=lambda x: x in {None})\n",
    "    )\n",
    "\n",
    "    return pd.Series(data=[i_results_session], index=[\"results_session\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_train_vocab = (\n",
    "    dataset_train.groupby([\"id_vocab\", \"language_asked\"])\n",
    "    .apply(lambda x: define_word_language_sessions(x, nb_sessions))\n",
    "    .reset_index()\n",
    ")\n",
    "dataset_train_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/raw/20201105/dataset_train_journey.pkl\", \"wb\") as file:\n",
    "    dill.dump(dataset_train_vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from more_itertools import locate\n",
    "\n",
    "\n",
    "def multiply_word_language_sessions(i_dataset_vocab):\n",
    "\n",
    "    i_results_session = i_dataset_vocab[\"results_session\"].tolist()[0]\n",
    "\n",
    "    all_results_session = [\n",
    "        (i_results_session[:i], i_results_session[i])\n",
    "        for i in locate(i_results_session, lambda x: x != None)\n",
    "    ]\n",
    "\n",
    "    session_before = [x[0] for x in all_results_session]\n",
    "    session_result = [x[1] for x in all_results_session]\n",
    "\n",
    "    return pd.DataFrame.from_dict({\"before\": session_before, \"result\": session_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_train_vocab_multiplied = (\n",
    "    dataset_train_vocab.groupby([\"id_vocab\", \"language_asked\"])\n",
    "    .apply(multiply_word_language_sessions)\n",
    "    .reset_index()\n",
    ")\n",
    "del dataset_train_vocab_multiplied['level_2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_train_vocab_multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/raw/20201105/dataset_train_journey_multiplied.pkl\", \"wb\") as file:\n",
    "    dill.dump(dataset_train_vocab_multiplied, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from src.features.make_sessions import (\n",
    "    define_word_language_sessions,\n",
    "    multiply_word_language_sessions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nb_sessions_inference = (\n",
    "    max(dataset_valid[\"id_session\"]) + 1\n",
    ")  # because it exists id_session = 0\n",
    "nb_sessions_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_word_language_sessions_valid = (\n",
    "    dataset_train_valid.groupby([\"id_vocab\", \"language_asked\"])\n",
    "    .apply(lambda x: define_word_language_sessions(x, nb_sessions_inference))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_valid = dataset_valid[\n",
    "    dataset_valid.groupby([\"id_vocab\", \"language_asked\"])[\"id_session\"].transform(max)\n",
    "    == dataset_valid[\"id_session\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_valid = pd.merge(\n",
    "    dataset_valid,\n",
    "    dataset_word_language_sessions_valid,\n",
    "    on=[\"id_vocab\", \"language_asked\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: we have to create an array of sub-arrays. Each sub-arrays will have X elements: the last failures/successes. We replace all non-existing elements by None for starters.\n",
    "\n",
    "* ~~1st test:~~ keeping None, 0, 1. We don't apply preprocessing -> **None is not supported**\n",
    "* 2nd test if fails: recode them as -1=failure, 0=None, 1=success\n",
    "\n",
    "We transform it to tensor.\n",
    "\n",
    "To this tensor, we apply shuffle, batch and repeat.\n",
    "\n",
    "We apply the same for validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.features.make_sessions as make_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### sessions_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_sessions = dataset_train[\"before\"].map(len).max()\n",
    "\n",
    "# add None to complete sessions\n",
    "dataset_train[\"sessions_standardized\"] = dataset_train[\"before\"].map(\n",
    "    lambda x: make_sessions.standardize_sessions(x, max_sessions)\n",
    ")\n",
    "\n",
    "dataset_train[\"sessions_numeric\"] = dataset_train[\"sessions_standardized\"].map(\n",
    "    make_sessions.map_session_to_numeric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add None to complete sessions\n",
    "dataset_valid[\"sessions_standardized\"] = dataset_valid[\"before\"].map(\n",
    "    lambda x: make_sessions.standardize_sessions(x, max_sessions)\n",
    ")\n",
    "\n",
    "dataset_valid[\"sessions_numeric\"] = dataset_valid[\"sessions_standardized\"].map(\n",
    "    make_sessions.map_session_to_numeric\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### array of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transform to array of array\n",
    "sessions_train = np.array(dataset_train[\"sessions_numeric\"].tolist(), dtype=\"int8\")\n",
    "sessions_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "targets_train = np.array(dataset_train[\"result\"].tolist(), dtype=\"int8\")  # to try: bool\n",
    "targets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transform to array of array\n",
    "sessions_valid = np.array(dataset_valid[\"sessions_numeric\"].tolist(), dtype=\"int8\")\n",
    "sessions_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "targets_valid = np.array(dataset_valid[\"result\"].tolist(), dtype=\"int8\")  # to try: bool\n",
    "targets_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into batch\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "ds_train = (\n",
    "    tf.data.Dataset.from_tensor_slices((sessions_train, targets_train))\n",
    "    .shuffle(buffer_size=10 * batch_size)\n",
    "    .repeat(num_epochs)\n",
    "    .batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_valid = tf.data.Dataset.from_tensor_slices((sessions_valid, targets_valid)).batch(\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_test = (8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(\n",
    "            10, activation=\"relu\", input_shape=input_shape_test\n",
    "        ),  # input shape required\n",
    "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to measure during epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = tf.keras.metrics.Mean(name=\"loss_train\")\n",
    "accuracy_train = tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy_train\")\n",
    "\n",
    "loss_valid = tf.keras.metrics.Mean(name=\"loss_valid\")\n",
    "accuracy_valid = tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy_valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sessions, labels):\n",
    "\n",
    "    # tf.GradientTape - Record operations for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = nn_model(sessions, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, nn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, nn_model.trainable_variables))\n",
    "\n",
    "    loss_train(loss)\n",
    "    accuracy_train(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(sessions, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = nn_model(sessions, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    loss_valid(t_loss)\n",
    "    accuracy_valid(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"loss_train\": [],\n",
    "    \"accuracy_train\": [],\n",
    "    \"loss_valid\": [],\n",
    "    \"accuracy_valid\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    loss_train.reset_states()\n",
    "    accuracy_train.reset_states()\n",
    "    # test\n",
    "    loss_valid.reset_states()\n",
    "    accuracy_valid.reset_states()\n",
    "\n",
    "    for sessions, labels in ds_train:\n",
    "        train_step(sessions, labels)\n",
    "\n",
    "    for test_sessions, test_labels in ds_valid:\n",
    "        test_step(test_sessions, test_labels)\n",
    "\n",
    "    # End epoch\n",
    "    results[\"loss_train\"].append(loss_train.result())\n",
    "    results[\"accuracy_train\"].append(accuracy_train.result())\n",
    "    results[\"loss_valid\"].append(loss_valid.result())\n",
    "    results[\"accuracy_valid\"].append(accuracy_valid.result())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, \\n\"\n",
    "        f\"Loss: {loss_train.result()}, \\n\"\n",
    "        f\"Accuracy: {accuracy_train.result() * 100},\\n\"\n",
    "        f\"Test Loss: {loss_valid.result()}, \\n\"\n",
    "        f\"Test Accuracy: {accuracy_valid.result() * 100}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(results[\"loss_train\"]))),\n",
    "        y=results[\"loss_train\"],\n",
    "        mode=\"lines\",  # ['markers', 'lines']\n",
    "        name=\"loss_train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(results[\"loss_valid\"]))),\n",
    "        y=results[\"loss_valid\"],\n",
    "        mode=\"lines\",  # ['markers', 'lines']\n",
    "        name=\"loss_valid\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Loss train vs valid per epoch\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"loss\",\n",
    "    legend={\"itemsizing\": \"constant\"},\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(results[\"accuracy_train\"]))),\n",
    "        y=results[\"accuracy_train\"],\n",
    "        mode=\"lines\",  # ['markers', 'lines']\n",
    "        name=\"accuracy_train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(results[\"accuracy_valid\"]))),\n",
    "        y=results[\"accuracy_valid\"],\n",
    "        mode=\"lines\",  # ['markers', 'lines']\n",
    "        name=\"accuracy_valid\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy train vs valid per epoch\",\n",
    "    xaxis_title=\"epoch\",\n",
    "    yaxis_title=\"accuracy\",\n",
    "    legend={\"itemsizing\": \"constant\"},\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "vocabulary_learning__simple_rnn",
   "language": "python",
   "name": "vocabulary_learning__simple_rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
